# ğŸ† DenseNet PyTorch Implementation

This repository contains a replication of **DenseNet (Densely Connected Convolutional Networks)** using PyTorch. The goal is to reproduce the **DenseNet-121 architecture** with dense and transition layers for ImageNet classification.

- Only **DenseNet-121** has been implemented.  
- Architecture follows **Conv1 â†’ DenseBlock1 â†’ Transition1 â†’ DenseBlock2 â†’ Transition2 â†’ DenseBlock3 â†’ Transition3 â†’ DenseBlock4 â†’ AvgPool â†’ Flatten â†’ FC** sequence.  
**Paper**: [Densely Connected Convolutional Networks (CVPR 2017)](https://arxiv.org/abs/1608.06993)

> ğŸ› ï¸ Users may need to adjust the code slightly to implement other DenseNet variants (DenseNet-169, 201, 264) or custom architectures.
---
## ğŸ–¼ Overview â€“ DenseNet Architecture

![DenseNet Overview](images/figuremix.jpg)  

- **Figure 1:** Dense connectivity pattern. Each layer receives feature-maps from all previous layers via concatenation, which improves gradient flow and enables feature reuse.  
- **Figure 2:** Transition layers between dense blocks. These layers reduce the spatial size of feature-maps using 2Ã—2 average pooling and can also compress the number of channels to keep the network efficient.  
- **Table 1:** DenseNet-121 configuration, showing the number of layers in each dense block and the growth rate for new feature-maps.

> DenseNet is a convolutional neural network that connects each layer to all previous layers, letting it reuse features and improve gradient flow. Inside each dense block, layers usually do batch normalization, ReLU activation, and a 3Ã—3 convolution. Optional 1Ã—1 bottleneck layers can reduce input size to make computation lighter. Between dense blocks, transition layers shrink the feature-map size and can compress channels. At the end, global average pooling feeds into a fully connected layer to produce the final predictions. This design makes DenseNet efficient, compact, and good for large-scale image recognition.

---

## ğŸ— Project Structure

```bash
DenseNet-Replicating/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ dense_layer.py         # Single dense layer: BN â†’ ReLU â†’ Conv3x3 + concat previous feature-maps
â”‚   â”‚   â”œâ”€â”€ transition_layer.py    # 1x1 Conv + AvgPooling (reduces feature-map size)
â”‚   â”‚   â”œâ”€â”€ conv1.py               # Initial 7x7 Conv + BN + ReLU
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py   # MaxPool after conv1
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py   # Global Average Pooling after last block
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py       # Conv â†’ FC transition
â”‚   â”‚   â””â”€â”€ fc_layer.py            # Fully Connected Layer (1000 classes)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ densenet121.py         # Full DenseNet-121: Conv1 â†’ DenseBlocks â†’ Transitions â†’ AvgPool â†’ Flatten â†’ FC
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # Hyperparameters
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)

